{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class FeedForwardNetwork(tf.keras.models.Model):\n",
    "    \"\"\"\n",
    "    Transformer用のPosition-wise Feedforward Neural Networkです。\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, dropout_rate: float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.filter_dense_layer = tf.keras.layers.Dense(hidden_dim * 4, use_bias=True,\n",
    "                                                        activation=tf.nn.relu, name='filter_layer')\n",
    "        self.output_dense_layer = tf.keras.layers.Dense(hidden_dim, use_bias=True, name='output_layer')\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, input: tf.Tensor, training: bool) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        FeedForwardNetwork を適用します。\n",
    "        :param input: shape = [batch_size, length, hidden_dim]\n",
    "        :return: shape = [batch_size, length, hidden_dim]\n",
    "        \"\"\"\n",
    "        tensor = self.filter_dense_layer(input)\n",
    "        tensor = self.dropout_layer(tensor, training=training)\n",
    "        return self.output_dense_layer(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def build(self, input_shape: tf.TensorShape) -> None:\n",
    "        hidden_dim = input_shape[-1]\n",
    "        self.scale = self.add_weight('layer_norm_scale', shape=[hidden_dim],\n",
    "                                     initializer=tf.ones_initializer())\n",
    "        self.bias = self.add_weight('layer_norm_bias', [hidden_dim],\n",
    "                                    initializer=tf.zeros_initializer())\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x: tf.Tensor, epsilon: float = 1e-6) -> tf.Tensor:\n",
    "        mean = tf.reduce_mean(x, axis=[-1], keepdims=True)\n",
    "        variance = tf.reduce_mean(tf.square(x - mean), axis=[-1], keepdims=True)\n",
    "        norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n",
    "\n",
    "        return norm_x * self.scale + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualNormalizationWrapper(tf.keras.models.Model):\n",
    "    def __init__(self, layer: tf.keras.layers.Layer, dropout_rate: float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layer = layer\n",
    "        self.layer_normalization = LayerNormalization()\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, input: tf.Tensor, training: bool, *args, **kwargs) -> tf.Tensor:\n",
    "        tensor = self.layer_normalization(input)\n",
    "        tensor = self.layer(tensor, training=training, *args, **kwargs)\n",
    "        tensor = self.dropout_layer(tensor, training=training)\n",
    "        return input + tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
