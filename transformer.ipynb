{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "willing-stylus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/hiroki/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FeedForwardNetwork' from 'common_layer' (./transformer/common_layer.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6514df849908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./transformer/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeedForwardNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResidualNormalizationWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAddPositionalEncoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSelfAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FeedForwardNetwork' from 'common_layer' (./transformer/common_layer.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path.append(\"./transformer/\")\n",
    "from common_layer import FeedForwardNetwork, ResidualNormalizationWrapper, LayerNormalization\n",
    "from embedding import AddPositionalEncoding, TokenEmbedding\n",
    "from attention import MultiheadAttention, SelfAttention\n",
    "from metrics import padded_cross_entropy_loss, padded_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "theoretical-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.models.Model):\n",
    "    '''\n",
    "    トークン列をベクトル列にエンコードする Encoder です。\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hopping_num: int,\n",
    "        head_num: int,\n",
    "        hidden_dim: int,\n",
    "        dropout_rate: float,\n",
    "        max_length: int,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hopping_num = hopping_num\n",
    "        self.head_num = head_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.token_embedding = TokenEmbedding(vocab_size, hidden_dim)\n",
    "        self.add_position_embedding = AddPositionalEncoding()\n",
    "        self.input_dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.attention_block_list: List[List[tf.keras.models.Model]] = []\n",
    "        \n",
    "        for _ in range(hopping_num):\n",
    "            attention_layer = SelfAttention(hidden_dim, head_num, dropout_rate, name='self_attention')\n",
    "            ffn_layer = FeedForwardNetwork(hidden_dim, dropout_rate, name='ffn')\n",
    "            self.attention_block_list.append([\n",
    "                ResidualNormalizationWrapper(attention_layer, dropout_rate, name='self_attention_wrapper'),\n",
    "                ResidualNormalizationWrapper(ffn_layer, dropout_rate, name='ffn_wrapper'),\n",
    "            ])\n",
    "        self.output_normalization = LayerNormalization()\n",
    "        \n",
    "    def call(\n",
    "            self,\n",
    "            input: tf.Tensor,\n",
    "            self_attention_mask: tf.Tensor,\n",
    "            training: bool,\n",
    "    ) -> tf.Tensor:\n",
    "        '''\n",
    "        モデルを実行します\n",
    "\n",
    "        :param input: shape = [batch_size, length]\n",
    "        :param training: 学習時は True\n",
    "        :return: shape = [batch_size, length, hidden_dim]\n",
    "        '''\n",
    "        # [batch_size, length, hidden_dim]\n",
    "        embedded_input = self.token_embedding(input)\n",
    "        embedded_input = self.add_position_embedding(embedded_input)\n",
    "        query = self.input_dropout_layer(embedded_input, training=training)\n",
    "        for i, layers in enumerate(self.attention_block_list):\n",
    "            attention_layer, ffn_layer = tuple(layers)\n",
    "            with tf.name_scope(f'hopping_{i}'):\n",
    "                query = attention_layer(query, attention_mask=self_attention_mask, training=training)\n",
    "                query = ffn_layer(query, training=training)\n",
    "        # [batch_size, length, hidden_dim]\n",
    "        return self.output_normalization(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "processed-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.models.Model):\n",
    "    '''\n",
    "    エンコードされたベクトル列からトークン列を生成する Decoder です。\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            hopping_num: int,\n",
    "            head_num: int,\n",
    "            hidden_dim: int,\n",
    "            dropout_rate: float,\n",
    "            max_length: int,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hopping_num = hopping_num\n",
    "        self.head_num = head_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, hidden_dim)\n",
    "        self.add_position_embedding = AddPositionalEncoding()\n",
    "        self.input_dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.attention_block_list: List[List[tf.keras.models.Model]] = []\n",
    "        for _ in range(hopping_num):\n",
    "            self_attention_layer = SelfAttention(hidden_dim, head_num, dropout_rate, name='self_attention')\n",
    "            enc_dec_attention_layer = MultiheadAttention(hidden_dim, head_num, dropout_rate, name='enc_dec_attention')\n",
    "            ffn_layer = FeedForwardNetwork(hidden_dim, dropout_rate, name='ffn')\n",
    "            self.attention_block_list.append([\n",
    "                ResidualNormalizationWrapper(self_attention_layer, dropout_rate, name='self_attention_wrapper'),\n",
    "                ResidualNormalizationWrapper(enc_dec_attention_layer, dropout_rate, name='enc_dec_attention_wrapper'),\n",
    "                ResidualNormalizationWrapper(ffn_layer, dropout_rate, name='ffn_wrapper'),\n",
    "            ])\n",
    "        self.output_normalization = LayerNormalization()\n",
    "        self.output_dense_layer = tf.keras.layers.Dense(vocab_size, use_bias=False)\n",
    "    \n",
    "    def call(\n",
    "            self,\n",
    "            input: tf.Tensor,\n",
    "            encoder_output: tf.Tensor,\n",
    "            self_attention_mask: tf.Tensor,\n",
    "            enc_dec_attention_mask: tf.Tensor,\n",
    "            training: bool,\n",
    "    ) -> tf.Tensor:\n",
    "        '''\n",
    "        モデルを実行します\n",
    "\n",
    "        :param input: shape = [batch_size, length]\n",
    "        :param training: 学習時は True\n",
    "        :return: shape = [batch_size, length, hidden_dim]\n",
    "        '''\n",
    "        # [batch_size, length, hidden_dim]\n",
    "        embedded_input = self.token_embedding(input)\n",
    "        embedded_input = self.add_position_embedding(embedded_input)\n",
    "        query = self.input_dropout_layer(embedded_input, training=training)\n",
    "        \n",
    "        for i, layers in enumerate(self.attention_block_list):\n",
    "            self_attention_layer, enc_dec_attention_layer, ffn_layer = tuple(layers)\n",
    "            with tf.name_scope(f'hopping_{i}'):\n",
    "                query = self_attention_layer(query, attention_mask=self_attention_mask, training=training)\n",
    "                query = enc_dec_attention_layer(query, memory=encoder_output,\n",
    "                                                attention_mask=enc_dec_attention_mask, training=training)\n",
    "                query = ffn_layer(query, training=training)\n",
    "\n",
    "        query = self.output_normalization(query)  # [batch_size, length, hidden_dim]\n",
    "        return self.output_dense_layer(query)  # [batch_size, length, vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-supervisor",
   "metadata": {},
   "source": [
    "vocab_sizeはなんのサイズだ？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pressing-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PAD_ID = 0\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.models.Model):\n",
    "    '''\n",
    "    Transformer モデルです。\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            hopping_num: int = 4,\n",
    "            head_num: int = 8,\n",
    "            hidden_dim: int = 512,\n",
    "            dropout_rate: float = 0.1,\n",
    "            max_length: int = 200,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hopping_num = hopping_num\n",
    "        self.head_num = head_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hopping_num=hopping_num,\n",
    "            head_num=head_num,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout_rate=dropout_rate,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hopping_num=hopping_num,\n",
    "            head_num=head_num,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout_rate=dropout_rate,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "    \n",
    "    def build_graph(self, name='transformer') -> None:\n",
    "        '''\n",
    "        学習/推論のためのグラフを構築します。\n",
    "        '''\n",
    "        with tf.name_scope(name):\n",
    "            self.is_training = tf.placeholder(dtype=tf.bool, name='is_training')\n",
    "            # [batch_size, max_length]\n",
    "            self.encoder_input = tf.placeholder(dtype=tf.int32, shape=[None, None], name='encoder_input')\n",
    "            # [batch_size]\n",
    "            self.decoder_input = tf.placeholder(dtype=tf.int32, shape=[None, None], name='decoder_input')\n",
    "\n",
    "            logit = self.call(\n",
    "                encoder_input=self.encoder_input,\n",
    "                decoder_input=self.decoder_input[:, :-1],  # 入力は EOS を含めない\n",
    "                training=self.is_training,\n",
    "            )\n",
    "            decoder_target = self.decoder_input[:, 1:]  # 出力は BOS を含めない\n",
    "\n",
    "            self.prediction = tf.nn.softmax(logit, name='prediction')\n",
    "\n",
    "            with tf.name_scope('metrics'):\n",
    "                xentropy, weights = padded_cross_entropy_loss(\n",
    "                    logit, decoder_target, smoothing=0.05, vocab_size=self.vocab_size)\n",
    "                self.loss = tf.identity(tf.reduce_sum(xentropy) / tf.reduce_sum(weights), name='loss')\n",
    "\n",
    "                accuracies, weights = padded_accuracy(logit, decoder_target)\n",
    "                self.acc = tf.identity(tf.reduce_sum(accuracies) / tf.reduce_sum(weights), name='acc')\n",
    "\n",
    "    def call(self, encoder_input: tf.Tensor, decoder_input: tf.Tensor, training: bool) -> tf.Tensor:\n",
    "        enc_attention_mask = self._create_enc_attention_mask(encoder_input)\n",
    "        dec_self_attention_mask = self._create_dec_self_attention_mask(decoder_input)\n",
    "\n",
    "        encoder_output = self.encoder(\n",
    "            encoder_input,\n",
    "            self_attention_mask=enc_attention_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        decoder_output = self.decoder(\n",
    "            decoder_input,\n",
    "            encoder_output,\n",
    "            self_attention_mask=dec_self_attention_mask,\n",
    "            enc_dec_attention_mask=enc_attention_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        return decoder_output\n",
    "\n",
    "    def _create_enc_attention_mask(self, encoder_input: tf.Tensor):\n",
    "        with tf.name_scope('enc_attention_mask'):\n",
    "            batch_size, length = tf.unstack(tf.shape(encoder_input))\n",
    "            pad_array = tf.equal(encoder_input, PAD_ID)  # [batch_size, m_length]\n",
    "            # shape broadcasting で [batch_size, head_num, (m|q)_length, m_length] になる\n",
    "            return tf.reshape(pad_array, [batch_size, 1, 1, length])\n",
    "\n",
    "    def _create_dec_self_attention_mask(self, decoder_input: tf.Tensor):\n",
    "        with tf.name_scope('dec_self_attention_mask'):\n",
    "            batch_size, length = tf.unstack(tf.shape(decoder_input))\n",
    "            pad_array = tf.equal(decoder_input, PAD_ID)  # [batch_size, m_length]\n",
    "            pad_array = tf.reshape(pad_array, [batch_size, 1, 1, length])\n",
    "\n",
    "            autoregression_array = tf.logical_not(\n",
    "                tf.matrix_band_part(tf.ones([length, length], dtype=tf.bool), -1, 0))  # 下三角が False\n",
    "            autoregression_array = tf.reshape(autoregression_array, [1, 1, length, length])\n",
    "\n",
    "            return tf.logical_or(pad_array, autoregression_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-carol",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
