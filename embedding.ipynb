{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dominant-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "favorite-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEncoding(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    入力テンソルに対し、位置の情報を付与して返すレイヤーです。\n",
    "    see: https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "    PE_{pos, 2i}   = sin(pos / 10000^{2i / d_model})\n",
    "    PE_{pos, 2i+1} = cos(pos / 10000^{2i / d_model})\n",
    "    '''\n",
    "    def call(self, input: tf.Tensor) -> tf.Tensor:\n",
    "        fl_type = inputs.dtype\n",
    "        batch_size, max_length, depth = tf.unstack(tf.shape(inputs))\n",
    "\n",
    "        depth_counter = tf.range(depth) // 2 * 2  # 0, 0, 2, 2, 4, ...\n",
    "        depth_matrix = tf.tile(tf.expand_dims(depth_counter, 0), [max_length, 1])  # [max_length, depth]\n",
    "        depth_matrix = tf.pow(10000.0, tf.cast(depth_matrix / depth, fl_type))  # [max_length, depth]\n",
    "\n",
    "        # cos(x) == sin(x + π/2)\n",
    "        phase = tf.cast(tf.range(depth) % 2, fl_type) * math.pi / 2  # 0, π/2, 0, π/2, ...\n",
    "        phase_matrix = tf.tile(tf.expand_dims(phase, 0), [max_length, 1])  # [max_length, depth]\n",
    "\n",
    "        pos_counter = tf.range(max_length)\n",
    "        pos_matrix = tf.cast(tf.tile(tf.expand_dims(pos_counter, 1), [1, depth]), fl_type)  # [max_length, depth]\n",
    "\n",
    "        positional_encoding = tf.sin(pos_matrix / depth_matrix + phase_matrix)\n",
    "        # [batch_size, max_length, depth]\n",
    "        positional_encoding = tf.tile(tf.expand_dims(positional_encoding, 0), [batch_size, 1, 1])\n",
    "\n",
    "        return inputs + positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "respiratory-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "\n",
    "class TokenEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, dtype=tf.float32, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dtype_ = dtype\n",
    "\n",
    "    def build(self, input_shape: tf.TensorShape) -> None:\n",
    "        self.lookup_table = self.add_variable(\n",
    "            name='token_embedding',\n",
    "            shape=[self.vocab_size, self.embedding_dim],\n",
    "            dtype=self.dtype_,\n",
    "            initializer=tf.random_normal_initializer(0., self.embedding_dim ** -0.5),\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, input: tf.Tensor) -> tf.Tensor:\n",
    "        mask = tf.to_float(tf.not_equal(input, PAD_ID))\n",
    "        embedding = tf.nn.embedding_lookup(self.lookup_table, input)\n",
    "        embedding *= tf.expand_dims(mask, -1)  # 元々 PAD だった部分を0にする\n",
    "        return embedding * self.embedding_dim ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-vintage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
